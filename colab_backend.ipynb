{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SIH Sentiment Analysis - Colab Backend\n",
        "This notebook provides a Flask API backend for the sentiment analysis platform.\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run all cells in order\n",
        "2. Get the ngrok URL from the last cell\n",
        "3. Update your frontend with the ngrok URL\n",
        "4. Upload CSV files through the frontend!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install flask flask-cors pyngrok transformers torch pandas matplotlib wordcloud nltk scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import pandas as pd\n",
        "import io\n",
        "import base64\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from transformers import pipeline\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import re\n",
        "from pyngrok import ngrok\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('opinion_lexicon', quiet=True)\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "    print(\"‚úÖ NLTK data downloaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è NLTK download warning: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ML models\n",
        "print(\"üîÑ Loading ML models... This may take a few minutes...\")\n",
        "\n",
        "# Load sentiment analysis model\n",
        "sentiment_pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "print(\"‚úÖ Sentiment model loaded!\")\n",
        "\n",
        "# Load summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")\n",
        "print(\"‚úÖ Summarization model loaded!\")\n",
        "\n",
        "# Load urgency classification model\n",
        "urgency_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "print(\"‚úÖ Urgency classification model loaded!\")\n",
        "\n",
        "print(\"üéâ All models loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for analysis\n",
        "def sentiment_analysis(text: str) -> tuple:\n",
        "    \"\"\"Analyze sentiment of a single text\"\"\"\n",
        "    try:\n",
        "        result = sentiment_pipe(text)\n",
        "        return result[0][\"label\"], round(result[0][\"score\"], 2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in sentiment analysis: {e}\")\n",
        "        return \"NEUTRAL\", 0.0\n",
        "\n",
        "def generate_summary(text: str) -> str:\n",
        "    \"\"\"Generate summary for a text\"\"\"\n",
        "    try:\n",
        "        # Truncate text if too long\n",
        "        if len(text) > 1000:\n",
        "            text = text[:1000]\n",
        "        result = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
        "        return result[0][\"summary_text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in summarization: {e}\")\n",
        "        return \"Unable to generate summary\"\n",
        "\n",
        "def detect_urgency(text: str, sentiment_label: str) -> str:\n",
        "    \"\"\"Detect urgency level based on text and sentiment\"\"\"\n",
        "    try:\n",
        "        # If positive, no urgency\n",
        "        if sentiment_label == \"POSITIVE\":\n",
        "            return \"Not Applicable\"\n",
        "        \n",
        "        # Clean text\n",
        "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(text))\n",
        "        if not text.strip():\n",
        "            return \"minor\"\n",
        "\n",
        "        urgency_labels = [\"critical\", \"moderate\", \"minor\"]\n",
        "        result = urgency_classifier(text, urgency_labels)\n",
        "        return result[\"labels\"][0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in urgency detection: {e}\")\n",
        "        return \"minor\"\n",
        "\n",
        "def generate_wordcloud(text: str) -> str:\n",
        "    \"\"\"Generate word cloud and return as base64 encoded image\"\"\"\n",
        "    try:\n",
        "        # Clean text\n",
        "        text_clean = re.sub(r\"[^a-z\\s]\", \"\", text.lower())\n",
        "        \n",
        "        # Get opinion words\n",
        "        pos_words = set(opinion_lexicon.positive())\n",
        "        neg_words = set(opinion_lexicon.negative())\n",
        "        all_sentiment_words = pos_words.union(neg_words)\n",
        "        \n",
        "        # Keep only sentiment words\n",
        "        sentiment_tokens = [w for w in text_clean.split() if w in all_sentiment_words]\n",
        "        \n",
        "        # Filter with VADER for strong sentiment words\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "        filtered_tokens = [\n",
        "            w for w in sentiment_tokens \n",
        "            if abs(sia.polarity_scores(w)[\"compound\"]) >= 0.41\n",
        "        ]\n",
        "        \n",
        "        sentiment_text = \" \".join(filtered_tokens)\n",
        "        \n",
        "        if not sentiment_text.strip():\n",
        "            # Fallback to original text if no sentiment words found\n",
        "            sentiment_text = text_clean\n",
        "        \n",
        "        # Generate word cloud\n",
        "        wc = WordCloud(\n",
        "            width=1000, height=600,\n",
        "            background_color=\"white\",\n",
        "            colormap=\"viridis\",\n",
        "            prefer_horizontal=0.9,\n",
        "            max_words=80,\n",
        "            min_font_size=12,\n",
        "            contour_color=\"black\", \n",
        "            contour_width=1,\n",
        "            relative_scaling=0.5,\n",
        "            normalize_plurals=True\n",
        "        ).generate(sentiment_text)\n",
        "        \n",
        "        # Convert to base64\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        plt.imshow(wc, interpolation=\"bilinear\")\n",
        "        plt.axis(\"off\")\n",
        "        \n",
        "        # Save to bytes\n",
        "        img_buffer = io.BytesIO()\n",
        "        plt.savefig(img_buffer, format='png', bbox_inches='tight', dpi=150)\n",
        "        img_buffer.seek(0)\n",
        "        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()\n",
        "        plt.close()\n",
        "        \n",
        "        return img_base64\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating word cloud: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "\n",
        "@app.route('/')\n",
        "def health_check():\n",
        "    return jsonify({\n",
        "        \"message\": \"SIH Sentiment Analysis API is running!\",\n",
        "        \"status\": \"healthy\",\n",
        "        \"models_loaded\": True\n",
        "    })\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def analyze_csv():\n",
        "    try:\n",
        "        print(\"üìä Processing new analysis request...\")\n",
        "        \n",
        "        # Get the uploaded file\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({'error': 'No file provided'}), 400\n",
        "        \n",
        "        file = request.files['file']\n",
        "        if file.filename == '':\n",
        "            return jsonify({'error': 'No file selected'}), 400\n",
        "        \n",
        "        # Read CSV file\n",
        "        contents = file.read()\n",
        "        df = pd.read_csv(io.StringIO(contents.decode('utf-8')))\n",
        "        \n",
        "        print(f\"üìÅ Processing {len(df)} rows...\")\n",
        "        \n",
        "        # Check if 'review' column exists\n",
        "        if 'review' not in df.columns:\n",
        "            # Try common column names\n",
        "            possible_columns = ['comment', 'comments', 'text', 'feedback', 'response']\n",
        "            review_column = None\n",
        "            for col in possible_columns:\n",
        "                if col in df.columns:\n",
        "                    review_column = col\n",
        "                    break\n",
        "            \n",
        "            if review_column is None:\n",
        "                return jsonify({'error': 'No review column found. Please ensure your CSV has a review column.'}), 400\n",
        "            \n",
        "            df = df.rename(columns={review_column: 'review'})\n",
        "            print(f\"‚úÖ Found review data in column: {review_column}\")\n",
        "        \n",
        "        # Process each review\n",
        "        results = []\n",
        "        all_text = \"\"\n",
        "        \n",
        "        for idx, row in df.iterrows():\n",
        "            review_text = str(row['review'])\n",
        "            all_text += review_text + \" \"\n",
        "            \n",
        "            # Analyze sentiment\n",
        "            sentiment_label, sentiment_score = sentiment_analysis(review_text)\n",
        "            \n",
        "            # Generate summary\n",
        "            summary = generate_summary(review_text)\n",
        "            \n",
        "            # Detect urgency\n",
        "            urgency = detect_urgency(review_text, sentiment_label)\n",
        "            \n",
        "            results.append({\n",
        "                \"id\": str(idx),\n",
        "                \"originalComment\": review_text,\n",
        "                \"summary\": summary,\n",
        "                \"sentiment\": sentiment_label.lower(),\n",
        "                \"sentimentScore\": sentiment_score,\n",
        "                \"urgency\": urgency\n",
        "            })\n",
        "        \n",
        "        print(f\"‚úÖ Processed {len(results)} comments\")\n",
        "        \n",
        "        # Calculate overall statistics\n",
        "        sentiment_counts = {}\n",
        "        for result in results:\n",
        "            sentiment = result[\"sentiment\"]\n",
        "            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n",
        "        \n",
        "        # Generate word cloud\n",
        "        print(\"üé® Generating word cloud...\")\n",
        "        wordcloud_image = generate_wordcloud(all_text)\n",
        "        \n",
        "        # Prepare response\n",
        "        analysis_data = {\n",
        "            \"sentimentAnalysis\": {\n",
        "                \"positive\": sentiment_counts.get(\"positive\", 0),\n",
        "                \"negative\": sentiment_counts.get(\"negative\", 0),\n",
        "                \"neutral\": sentiment_counts.get(\"neutral\", 0),\n",
        "                \"totalComments\": len(results)\n",
        "            },\n",
        "            \"summaries\": results,\n",
        "            \"wordCloud\": {\n",
        "                \"image\": wordcloud_image,\n",
        "                \"format\": \"base64\"\n",
        "            },\n",
        "            \"urgencyAnalysis\": {\n",
        "                \"critical\": len([r for r in results if r[\"urgency\"] == \"critical\"]),\n",
        "                \"moderate\": len([r for r in results if r[\"urgency\"] == \"moderate\"]),\n",
        "                \"minor\": len([r for r in results if r[\"urgency\"] == \"minor\"]),\n",
        "                \"notApplicable\": len([r for r in results if r[\"urgency\"] == \"Not Applicable\"])\n",
        "            },\n",
        "            \"averageSentimentScore\": sum([r[\"sentimentScore\"] for r in results]) / len(results) if results else 0\n",
        "        }\n",
        "        \n",
        "        print(\"üéâ Analysis complete!\")\n",
        "        return jsonify(analysis_data)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing file: {e}\")\n",
        "        return jsonify({'error': f'Error processing file: {str(e)}'}), 500\n",
        "\n",
        "print(\"‚úÖ Flask app created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the Flask server with ngrok\n",
        "print(\"üöÄ Starting Flask server...\")\n",
        "print(\"üì° Setting up ngrok tunnel...\")\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\nüåê Your API is now available at: {public_url}\")\n",
        "print(f\"üîó Use this URL in your frontend: {public_url}/analyze\")\n",
        "print(\"\\nüìã Next steps:\")\n",
        "print(\"1. Copy the URL above\")\n",
        "print(\"2. Update your frontend with this URL\")\n",
        "print(\"3. Start your Next.js frontend\")\n",
        "print(\"4. Upload CSV files through the frontend!\")\n",
        "print(\"\\n‚ö†Ô∏è Keep this cell running to keep the API active!\")\n",
        "\n",
        "# Start the Flask app\n",
        "app.run(host='0.0.0.0', port=5000, debug=False)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
